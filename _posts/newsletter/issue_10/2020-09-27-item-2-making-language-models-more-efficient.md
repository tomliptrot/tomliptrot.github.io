---
date: 2020-09-27 21:00:00
image: /assets/images/rukma-pratista-9oEL0oQFDsA-unsplash.jpg
layout: post
link: https://www.arxiv-vanity.com/papers/2009.06732/
story_number: 2
title: Making language models more efficient
word_count: 10,903
---

Language models like GPT-3, Bert and others have recently transformed the filed of natural language processing. But they are often big and slow.

These models can do task like question answering, translation and language generation at near human levels. A big drawback of these models are that they are too big and slow to use. GPT-3 has 175 Billion parameters and is too big to run on all but the most expensive supercomputers.

This paper is a survey of the huge amount of research that has gone into making them more efficient.

