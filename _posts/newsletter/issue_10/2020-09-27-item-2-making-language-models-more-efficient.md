---
date: 2020-09-27 21:00:00
image: /assets/images/rukma-pratista-9oEL0oQFDsA-unsplash.jpg
layout: post
link: https://www.arxiv-vanity.com/papers/2009.06732/
story_number: 2
title: Making language models more efficient
word_count: 10,903
---

In the last few years, transformer based language models like GPT-3, Bert and others have revolutionised natural language processing.  Here is how to make the more efficient.

Language models are like the predictive text function on your mobile phone. Recent advances and massive amounts of compute and data have made them good at a range of tasks. These models can do tasks like question answering, translation and language generation at near human levels. However, a big drawback is that they are too big and slow to use. GPT-3 has 175 Billion parameters and is too large to run on all but the most expensive supercomputers.

This paper is a survey of the huge amount of research that has gone into making them more efficient. It provides a taxonomy for understanding the literature and an overview of the most promising approaches.

