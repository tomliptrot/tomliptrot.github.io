---
category: machinelearning
date: 2021-06-22 08:30:00
image: /assets/images/newsletter/issue_21/david-clode-Q-8jreAEa-4-unsplash.jpeg
layout: post
link: https://6b.eleuther.ai/
story_number: 5
title: Open super big language model
word_count: null
---

A new, open, alternative to the super big language model GPT-3 has been released. It wrote the following about itself:

> "rpwlcr = versatile automatic writing – Genesis writing loop based on Peter Overlefsholtz's formula inspired by and derived from MUDCon VoX scribble [Yes; it would be silly for GPT-3 to not even claim that – LD] used for learning to be GPT-3 creator Zihon Rusakowski ; good tools are eternal best biceps ; PrimalScribbler family sed (error recognition and corrective drafting, safe backups); BEGIN UPWARD pointing source prototype literate interpreters ALL MAY READ PROTOTYPE written for aspiring TP. XLIT given-alice paths allowed (eg: Paul Lewis paths; especially handkerchief's application of Katipussi fonts/following sounds to construct Polish apostrophe saving pronoun), even improving usability (Hmmm, N), or paradigm engineering (C, Homolateral exophony ). only practical contemporary POST written in 010, ten seconds – bps · in FRPEH R Q00 xxQ? S9 9993 IRNT AP1ED 15 36 00300 15 O37 B° jí2IMMìGIC à€ c00«JJ©R u é©~ êv "ú_ ®, a"

Hmm - that's not perfect, is it? A sick parrot?

🛎️ **Why this matters:** The singularity is far, but at least it's open.

