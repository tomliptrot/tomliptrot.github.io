---
category: philosophy
date: 2021-05-25 08:30:00
image: /assets/images/newsletter/issue_20/stephen-leonardi-XqzLUKzg49c-unsplash.jpeg
layout: post
link: https://www.project-syndicate.org/onpoint/how-to-think-about-existential-and-immediate-risks-by-daron-acemoglu-2021-05
story_number: 1
title: ARGH! AI is going to kill us all!
word_count: 2,454
---

The dinosaurs got killed off by an asteroid. We'll probably get killed off by ourselves.

There is a burgeoning tradition of prophecies by clever rich white men about the long term dangers of 'super-intelligent' AI. A new [book](https://www.amazon.co.uk/dp/B082SXGJYC/) on the topic has now been added by  Oxford philosopher Toby Ord and nicely [reviewed](https://www.project-syndicate.org/onpoint/how-to-think-about-existential-and-immediate-risks-by-daron-acemoglu-2021-05) by Daron Acemoglu.

The idea is like this: We've got power not wisdom, we're developed technology without the wisdom to control it. We should minimise existential risk whatever the cost. The greatest threat is AI. We should focus a large amount of current resources on avoiding there fate of the dinosaurs.

I don't buy it. Worrying about super intelligent AI taking over is (as Andrew Ng said) like worrying about overpopulation on Mars. We're not there yet.

What we should be working about is the impact existing AI + human chimeras and their corporate bodies are doing to the world today. Inequality, environmental damage, misinformation and control are all big present problems that would be more fruitful to focus on. Initiatives such as the EU's proposed AI regulation are likely to have more positive impact than philosophical warbling (much as I enjoy philosophical warbling)

üõéÔ∏è **Why this matters:** AI already runs the world.

